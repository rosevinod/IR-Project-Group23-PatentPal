{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_patent_retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIIIeMIaW8cC"
      },
      "source": [
        "\"\"\"\n",
        "Reading 1000 entries of the patent database and storing\n",
        "the descriptions in a list \n",
        "\"\"\"\n",
        "import pickle\n",
        "data = pickle.load(open(\"/content/drive/MyDrive/bigPatentData/data_pik\", 'rb'))\n",
        "#print(data[\"description\"])\n",
        "des = data[\"description\"][0:200].tolist()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdpeZN_-YvcI"
      },
      "source": [
        "\"\"\"\n",
        "Aplying preprocessing on the data\n",
        "\"\"\"\n",
        "\n",
        "#%cd \"drive/My Drive/Colab Notebooks\"\n",
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "\n",
        "from IR_Assignment1_preprocessing import preprocess\n",
        "\n",
        "preprocessed = []\n",
        "for i in des:\n",
        "  preprocessed.append(preprocess(i))\n",
        "\n",
        "print(preprocessed[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXtcsYpgcAuy"
      },
      "source": [
        "\"\"\"\n",
        "Basic inverted index construction\n",
        "format: term: doc1, doc2,..\n",
        "where doc1 is the position of the doc in the dataframe\n",
        "\"\"\"\n",
        "import collections\n",
        "index = {}\n",
        "\n",
        "numfiles = len(preprocessed)\n",
        "print(numfiles)\n",
        "for i in range(numfiles):\n",
        "  for term in preprocessed[i]:\n",
        "    if term not in index:\n",
        "      index[term] = [i]\n",
        "    elif (index[term])[-1]!=i:\n",
        "      (index[term]).append(i)\n",
        "\n",
        "index = collections.OrderedDict(sorted(index.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9SANHdakKh-"
      },
      "source": [
        "\"\"\"\n",
        "Dumping the index in a file\n",
        "\"\"\"\n",
        "%cd \"drive/My Drive\"\n",
        "with open('index.txt', 'wb') as fp:\n",
        "    pickle.dump(index, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dNEG348mjrK"
      },
      "source": [
        "\"\"\"\n",
        "Positional index construction\n",
        "\"\"\"\n",
        "import collections\n",
        "index2 = {} #dictionary of dictionaries (not for now)\n",
        "\n",
        "numfiles = len(preprocessed)\n",
        "print(numfiles)\n",
        "for i in range(numfiles):\n",
        "  occurences = {}\n",
        "  for t in range(len(preprocessed[i])):\n",
        "    term = preprocessed[i][t]\n",
        "    if term not in occurences:\n",
        "      occurences[term] = [t]\n",
        "    else:\n",
        "      occurences[term].append(t)\n",
        "\n",
        "  for term in occurences:\n",
        "    positions = occurences[term]\n",
        "    if term in index2:\n",
        "      index2[term].append((i, positions))\n",
        "      #index2[term][i] = positions\n",
        "    else:\n",
        "      index2[term] = ([i, positions)]\n",
        "\n",
        "index2 = collections.OrderedDict(sorted(index2.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI5EdAa9s9An"
      },
      "source": [
        "\"\"\"\n",
        "Dumping the index in a file\n",
        "\"\"\"\n",
        "with open('index2.txt', 'wb') as fp:\n",
        "    pickle.dump(index2, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEpY0P9Eyo3v"
      },
      "source": [
        "ind1 = pickle.load(open(\"/content/drive/MyDrive/index.txt\", 'rb'))\n",
        "print(ind1[\"contain\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBHtmr5y1D3Q"
      },
      "source": [
        "ind2 = pickle.load(open(\"/content/drive/MyDrive/index2.txt\", 'rb'))\n",
        "print(ind2[\"contain\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKjF5djD9kBT"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_docs(dict_, term):\n",
        "  if term not in dict_:\n",
        "    return np.array([])\n",
        "  return np.array(dict_[term])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfTwvYLo8fAr"
      },
      "source": [
        "\"\"\"\n",
        "AND query\n",
        "\"\"\"\n",
        "import time\n",
        "%cd \"drive/My Drive/Colab Notebooks\"\n",
        "from IR_Assignment1_preprocessing import preprocess\n",
        "\n",
        "def and_query(query):\n",
        "  start = time.time()\n",
        "  query = preprocess(query)\n",
        "\n",
        "  term1 = query[0]\n",
        "  docs1 = get_docs(ind1, term1)\n",
        "  n = len(query)\n",
        "  ans=[]\n",
        "  for i in range(n-1):\n",
        "    term2 = query[i+1]\n",
        "    docs2 = get_docs(ind1, term2)\n",
        "    i1 = 0\n",
        "    i2 = 0\n",
        "    ans = []\n",
        "    while i1 < len(docs1) and i2 < len(docs2):\n",
        "      if docs1[i1] == docs2[i2]:\n",
        "        ans.append(docs1[i1])\n",
        "        i1 += 1\n",
        "        i2 += 1\n",
        "      elif docs1[i1] < docs2[i2]:\n",
        "        i1 += 1\n",
        "      elif docs2[i2] < docs1[i1]:\n",
        "        i2 += 1\n",
        "    docs1 = [i for i in ans]\n",
        "  end = time.time()\n",
        "  print(\"Documents retrieved:\",ans)\n",
        "  print(\"Number of docs:\", len(ans))\n",
        "  print(\"Time taken:\", end-start)\n",
        "  return len(ans),end-start\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-TGl5V3GpRF"
      },
      "source": [
        "\"\"\"\n",
        "Cosine similarity Ranking\n",
        "\"\"\"\n",
        "import math\n",
        "import operator\n",
        "\n",
        "def cosine_similarity(query):\n",
        "  start = time.time()\n",
        "  query = preprocess(query)\n",
        "  scores = {}\n",
        "  length_query = 0\n",
        "\n",
        "  query_freq = {}\n",
        "  for i in query:\n",
        "    if i in query_freq:\n",
        "      query_freq[i]+=1\n",
        "    else:\n",
        "      query_freq[i]=1 \n",
        "\n",
        "  n = 10000\n",
        "  length_vector = {}\n",
        "  for query_term in query_freq:\n",
        "    term_tf = query_freq[query_term]\n",
        "    if query_term in ind2:\n",
        "      term_postings = ind2[query_term]\n",
        "      freq = len(term_postings)\n",
        "      idf = math.log10(float(n)/float(freq))\n",
        "      weight_query = term_tf * idf\n",
        "    else:\n",
        "      term_postings = []\n",
        "      weight_query = 0  \n",
        "    length_query += math.pow(weight_query, 2)\n",
        "    relevant_docs = []\n",
        "\n",
        "    for (doc_id, positions) in term_postings:\n",
        "      relevant_docs.append(doc_id)\n",
        "      tf = len(positions)\n",
        "      if tf==0:\n",
        "        log_tf = 0\n",
        "      else:\n",
        "        log_tf = 1+math.log(tf)\n",
        "      length_vector[doc_id] = length_vector.get(doc_id,0.0) + (log_tf*log_tf)\n",
        "      weight_d_t = float(tf)\n",
        "      if doc_id in scores:\n",
        "        scores[doc_id] += weight_d_t * weight_query\n",
        "      else:\n",
        "        scores[doc_id] = weight_d_t * weight_query\n",
        "\n",
        "  length_query = math.sqrt(length_query)\n",
        "      \n",
        "  for doc_id in relevant_docs:\n",
        "    scores[doc_id] = scores[doc_id]/(math.sqrt(length_vector[doc_id])*length_query)\n",
        "\n",
        "  ordered_scores = sorted(scores.items(), key=operator.itemgetter(1), reverse=True);\n",
        "  end = time.time()\n",
        "  n_sc = len(ordered_scores)\n",
        "  print(\"Documents retrieved:\",ordered_scores[0:min(25,n_sc)])\n",
        "  print(\"Number of docs:\", 20)\n",
        "  print(\"Time taken:\", end-start)\n",
        "  return end-start\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF22i0t5mwlV"
      },
      "source": [
        "ndocs = []\n",
        "andtimes = []\n",
        "costimes = []\n",
        "queries = [\"fan cold cooling rotate metal steel cover heating\", \"dentist typically supplies the technician with a full face photograph\", \"may be straight - chained or branched . an optionally substituted alkyl\", \"interchangeable retail display in accordance with the invention\", \"numerous specific details are set forth in order to provide a thorough\", \"effects of garlic extracts containing allicin for prostate tumor treatment\", \"cooking device 10 will now be described with respect to the figures\", \"figures are not drawn to scale and they are provided merely \", \"accordance with the present invention will be described\",\"drawings wherein like numerals refer to like matter throughout\"]\n",
        "for q in queries:\n",
        "  print(q)\n",
        "  print(\"And:\")\n",
        "  ndoc, andtime = and_query(q)\n",
        "  ndocs.append(ndoc)\n",
        "  andtimes.append(andtime)\n",
        "  print(\"Cosine Similarity\")\n",
        "  costime = cosine_similarity(q)\n",
        "  costimes.append(costime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdcaqdaWtdtl"
      },
      "source": [
        "print(ndocs)\n",
        "print(andtimes)\n",
        "print(costimes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj0Gx3ta5PZe"
      },
      "source": [
        "index2 = {}\n",
        "for i in ind2:\n",
        "  index2[i] = {}\n",
        "  for j in ind2[i]:\n",
        "    index2[i][j[0]]=j[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcWPZ0HEVLxx"
      },
      "source": [
        "\"\"\"\n",
        "Positional bigram query\n",
        "\"\"\"\n",
        "import time\n",
        "%cd \"drive/My Drive/Colab Notebooks\"\n",
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "\n",
        "from IR_Assignment1_preprocessing import preprocess\n",
        "\n",
        "def and_query(query):\n",
        "  start = time.time()\n",
        "  query = preprocess(query)\n",
        "\n",
        "  term1 = query[0]\n",
        "  docs1 = get_docs(ind1, term1)\n",
        "  n = len(query)\n",
        "  ans=[]\n",
        "  for i in range(n-1):\n",
        "    term2 = query[i+1]\n",
        "    docs2 = get_docs(ind1, term2)\n",
        "    i1 = 0\n",
        "    i2 = 0\n",
        "    ans = []\n",
        "    while i1 < len(docs1) and i2 < len(docs2):\n",
        "      if docs1[i1] == docs2[i2]:\n",
        "        pos1 = set(index2[term1][docs1[i1]])\n",
        "        pos2 = set(index2[term2][docs2[i2]])\n",
        "        for i in pos1:\n",
        "          if i+1 in pos2:\n",
        "            ans.append(docs1[i1])\n",
        "            i1 += 1\n",
        "            i2 += 1\n",
        "            break\n",
        "        i1+=1\n",
        "        i2+=1\n",
        "      elif docs1[i1] < docs2[i2]:\n",
        "        i1 += 1\n",
        "      elif docs2[i2] < docs1[i1]:\n",
        "        i2 += 1\n",
        "    term1 = term2\n",
        "    docs1 = [i for i in ans]\n",
        "  end = time.time()\n",
        "  print(\"Documents retrieved:\",ans)\n",
        "  print(\"Number of docs:\", len(ans))\n",
        "  print(\"Time taken:\", end-start)\n",
        "  return len(ans),end-start\n",
        "\n",
        "ndocs = []\n",
        "times = []\n",
        "queries = [\"fan cold cooling rotate metal steel cover heating\",\"dentist typically supplies the technician with a full face photograph\", \"may be straight - chained or branched . an optionally substituted alkyl\", \"interchangeable retail display in accordance with the invention\", \"numerous specific details are set forth in order to provide a thorough\", \"effects of garlic extracts containing allicin for prostate tumor treatment\", \"cooking device 10 will now be described with respect to the figures\", \"figures are not drawn to scale and they are provided merely \", \"accordance with the present invention will be described\",\"drawings wherein like numerals refer to like matter throughout\"]\n",
        "for q in queries:\n",
        "  print(q)\n",
        "  print(\"And:\")\n",
        "  ndoc, andtime = and_query(q)\n",
        "  ndocs.append(ndoc)\n",
        "  times.append(andtime)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}